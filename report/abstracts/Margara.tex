Many modern distributed software applications involve processing, analyzing, and reacting to potentially large volumes of streaming data. Examples of such applications include monitoring systems, decision support systems, financial analysis tools and traffic control systems.

Designing and implementing these applications is difficult. Indeed, the streaming nature of the data demands for efficient algorithms, techniques, and infrastructure to analyze the incoming information on the fly and extract relevant knowledge from it.

During this talk, I presented my experience in this area. I first focused on the design of a language explicitly conceived to identify situations of interest from large streams of low level data. Next, I discussed the implementation of efficient processing algorithms for streaming data that can exploit modern many-core architectures, such as multi-core CPUs and programmable GPUs, to reduce the processing latency and increase the overall throughput of the system. Finally, I presented the open issues and challenges in the field of stream processing, with emphasis on the integration within complex software architectures and the tradeoff between data consistency and data processing and management costs in such architectures.
